import os
import sys
import subprocess
import shutil
import json
import csv
import pandas as pd
from scipy.signal import butter, filtfilt
import cv2
import time

print("opencv_version", cv2.__version__)

# Set AlphaPose directory as working directory
alphapose_dir = r"C:\AlphaPose"
checkpoint_path = r"C:\AlphaPose\pretrained_models\pose_model.pth"
config_path = r"C:\AlphaPose\configs\coco\resnet\256x192_res50_lr1e-3_1x.yaml"
os.chdir(alphapose_dir)
sys.path.append(alphapose_dir)

base_video_dir = r"C:\Users\USER\OneDrive\Desktop_20250211_after_reset\åº·èˆ’_motion\recorded_videos"
skeleton_video_dir = r"C:\Users\USER\OneDrive\Desktop_20250211_after_reset\åº·èˆ’_motion\skeleton_videos"
json_dir = r"C:\Users\USER\OneDrive\Desktop_20250211_after_reset\åº·èˆ’_motion\joints_json_files"
csv_dir = r"C:\Users\USER\OneDrive\Desktop_20250211_after_reset\åº·èˆ’_motion\joints_csv_files"
h5_dir = r"C:\Users\USER\OneDrive\Desktop_20250211_after_reset\åº·èˆ’_motion\joints_h5_files"

# --- åˆå§‹åŒ–è³‡æ–™å¤¾çµæ§‹ ---
output_dirs = {
    "videos": base_video_dir,
    "skeleton": skeleton_video_dir,
    "json": json_dir,
    "csv": csv_dir,
    "h5": h5_dir
}

for name, path in output_dirs.items():
    os.makedirs(path, exist_ok=True)
    print(f"ğŸ“ {name} è³‡æ–™å¤¾æº–å‚™å®Œæˆï¼š{path}")



def build_output_paths_from_video(video_path):
    filename = os.path.basename(video_path)
    folder_name = os.path.splitext(filename)[0]
    output_paths = {
        "skeleton_video": os.path.join(skeleton_video_dir, folder_name),
        "json": os.path.join(json_dir, folder_name),
        "csv": os.path.join(csv_dir, folder_name),
        "h5": os.path.join(h5_dir, folder_name),
    }
    for path in output_paths.values():
        os.makedirs(path, exist_ok=True)
    return output_paths

def find_all_avi_files(base_dir):
    avi_files = []
    for root, dirs, files in os.walk(base_dir):
        for file in files:
            if file.endswith(".avi"):
                full_path = os.path.join(root, file)
                avi_files.append(full_path)
    return avi_files

video_paths_list = find_all_avi_files(base_video_dir)
print(f"âœ… å…±æ‰¾åˆ° {len(video_paths_list)} å€‹å½±ç‰‡ï¼š")

# --- Run AlphaPose on each video ---
for video_path in video_paths_list:
    output_file_name = os.path.splitext(os.path.basename(video_path))[0]
    output_json_path = os.path.join(json_dir, f"AlphaPose_{output_file_name}")

    command = [
        "python", os.path.join(alphapose_dir, "scripts", "demo_inference.py"),
        "--video", video_path,
        "--outdir", output_json_path,
        "--checkpoint", checkpoint_path,
        "--cfg", config_path,
        "--save_video",
        "--vis_fast",
        "--gpus", "0"
    ]

    try:
        print(f"\nğŸš€ Running AlphaPose on: {video_path}")
        subprocess.run(command, check=True)

        if os.path.exists(output_json_path):
            print(f"âœ… JSON folder created: {output_json_path}")
        else:
            print(f"âš ï¸ JSON result folder not found for: {output_json_path}")
    except subprocess.CalledProcessError as e:
        print(f"âŒ AlphaPose error on {video_path}: {e}")
        continue


"""
æ¬é·å½±ç‰‡å’Œ JSON æª”æ¡ˆåˆ°æŒ‡å®šè³‡æ–™å¤¾
ä¸¦åˆªé™¤åŸå§‹è³‡æ–™å¤¾
"""


# --- åˆå§‹åŒ–è³‡æ–™å¤¾çµæ§‹ ---
output_dirs = {
    "videos": base_video_dir,
    "skeleton": skeleton_video_dir,
    "json": json_dir,
    "csv": csv_dir,
    "h5": h5_dir
}

for name, path in output_dirs.items():
    os.makedirs(path, exist_ok=True)
    print(f"ğŸ“ {name} è³‡æ–™å¤¾æº–å‚™å®Œæˆï¼š{path}")

print("\nğŸ“¦ é–‹å§‹æ•´ç†è¼¸å‡ºè³‡æ–™å¤¾...")
failed_folders = []

for folder_name in os.listdir(json_dir):
    folder_path = os.path.join(json_dir, folder_name)
    if not os.path.isdir(folder_path):
        continue

    print(f"\nğŸ” è™•ç†è³‡æ–™å¤¾ï¼š{folder_path}")
    base_name = folder_name.replace("AlphaPose_output_", "")
    json_src = os.path.join(folder_path, "alphapose-results.json")
    json_dst = os.path.join(json_dir, f"AlphaPose_output_{base_name}.json")

    # é æœŸçš„ avi æª”å
    expected_avi = os.path.join(folder_path, f"{folder_name}.avi")
    avi_dst = os.path.join(skeleton_video_dir, f"AlphaPose_output_{base_name}.avi")

    # æª¢æŸ¥ avi å­˜åœ¨èˆ‡å¦ï¼Œè‹¥æ‰¾ä¸åˆ°å°± fallback æƒææ‰€æœ‰ avi
    avi_src = None
    if os.path.exists(expected_avi):
        avi_src = expected_avi
        print(f"âœ… æ‰¾åˆ°é æœŸ aviï¼š{avi_src}")
    else:
        print(f"âš ï¸ æ‰¾ä¸åˆ°é æœŸ aviï¼š{expected_avi}ï¼Œé–‹å§‹æƒæè³‡æ–™å¤¾å…§å®¹...")
        for f in os.listdir(folder_path):
            if f.lower().endswith(".avi"):
                avi_src = os.path.join(folder_path, f)
                print(f"ğŸ” æ‰¾åˆ°å…¶ä»– aviï¼š{avi_src}")
                break

    try:
        if avi_src and os.path.exists(avi_src) and os.path.getsize(avi_src) > 0:
            os.rename(avi_src, avi_dst)
            print(f"ğŸ¥ ç§»å‹• avi åˆ°ï¼š{avi_dst}")
        else:
            print(f"âš ï¸ ç„¡æ³•è™•ç† aviï¼š{avi_src}ï¼ˆä¸å­˜åœ¨æˆ–ç‚ºç©ºï¼‰")

        if os.path.exists(json_src):
            os.rename(json_src, json_dst)
            print(f"ğŸ“„ ç§»å‹• JSON åˆ°ï¼š{json_dst}")
        else:
            print(f"âš ï¸ æ‰¾ä¸åˆ° JSONï¼š{json_src}")

        time.sleep(0.5)
        shutil.rmtree(folder_path)
        print(f"ğŸ§¹ å·²åˆªé™¤è³‡æ–™å¤¾ï¼š{folder_path}")

    except Exception as e:
        print(f"âŒ ç„¡æ³•è™•ç†ï¼š{folder_path}ï¼ŒéŒ¯èª¤ï¼š{e}")
        failed_folders.append(folder_path)







"""
è½‰jsonåˆ°csv
"""
coco_keypoints = [
    "Nose", "LEye", "REye", "LEar", "REar",
    "LShoulder", "RShoulder", "LElbow", "RElbow",
    "LWrist", "RWrist", "LHip", "RHip",
    "LKnee", "RKnee", "LAnkle", "RAnkle"
]

# --- Apply Butterworth Filter ---
def butterworth_filter(data, cutoff=3, fs=30, order=2):
    if len(data) < order * 3:
        return data
    nyq = 0.5 * fs
    norm_cutoff = cutoff / nyq
    b, a = butter(order, norm_cutoff, btype='low', analog=False)
    return filtfilt(b, a, data)

# --- è½‰æ›æ‰€æœ‰ JSON æª”ç‚º CSV ---
print("\nğŸ“„ é–‹å§‹è½‰æ› JSON ç‚º CSV...")

for file in os.listdir(json_dir):
    if not file.endswith(".json"):
        continue

    json_path = os.path.join(json_dir, file)
    csv_path = os.path.join(csv_dir, file.replace(".json", ".csv"))
    print(f"\nğŸ” è™•ç†æª”æ¡ˆï¼š{json_path}")

    try:
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        if not data:
            print(f"âš ï¸ ç©º JSONï¼š{json_path}")
            continue

        # å»ºç«‹ä¸€å€‹ dictï¼škey ç‚º joint åï¼Œå€¼ç‚º (xåºåˆ—, yåºåˆ—)
        keypoint_series = {kp: {"x": [], "y": []} for kp in coco_keypoints}

        for item in data:
            kp_list = item["keypoints"]
            for i, kp in enumerate(coco_keypoints):
                x = kp_list[i * 3]
                y = kp_list[i * 3 + 1]
                keypoint_series[kp]["x"].append(x)
                keypoint_series[kp]["y"].append(y)

        # æ¿¾æ³¢è™•ç†å¾Œå»ºç«‹ dataframe
        filtered_data = {}
        for kp in coco_keypoints:
            filtered_x = butterworth_filter(keypoint_series[kp]["x"])
            filtered_y = butterworth_filter(keypoint_series[kp]["y"])
            filtered_data[f"{kp}_x"] = filtered_x
            filtered_data[f"{kp}_y"] = filtered_y

        df = pd.DataFrame(filtered_data)
        df.to_csv(csv_path, index=False, encoding="utf-8-sig")
        print(f"âœ… å·²è¼¸å‡º CSVï¼š{csv_path}")

    except Exception as e:
        print(f"âŒ è™•ç†å¤±æ•—ï¼š{json_path}ï¼ŒéŒ¯èª¤ï¼š{e}")




"""
è½‰jsonåˆ°h5
"""
print("\nğŸ“ é–‹å§‹è½‰æ› JSON ç‚º HDF5 æª”æ¡ˆ...")

# å®šç¾© COCO keypoints é †åºèˆ‡ HDF5 MultiIndex çµæ§‹

scorer = "AlphaPose"
coords = ["x", "y", "likelihood"]
multi_index = pd.MultiIndex.from_product([[scorer], coco_keypoints, coords], names=["scorer", "bodyparts", "coords"])



# è™•ç†æ¯å€‹ JSON æª”æ¡ˆ
for json_file in os.listdir(json_dir):
    if not json_file.endswith(".json"):
        continue

    json_path = os.path.join(json_dir, json_file)
    h5_output_path = os.path.join(h5_dir, json_file.replace(".json", "_filtered.h5"))

    print(f"\nğŸ” è™•ç†æª”æ¡ˆï¼š{json_path}")

    try:
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        if not data:
            print(f"âš ï¸ ç©º JSONï¼š{json_path}")
            continue

        keypoints_list = [entry["keypoints"] for entry in data]
        num_joints = len(coco_keypoints)

        # åŸºæœ¬é©—è­‰
        assert len(keypoints_list[0]) == num_joints * 3, f"âŒ JSON çµæ§‹éŒ¯èª¤ï¼š{json_file}"

        # å»ºç«‹ DataFrameï¼ˆMultiIndexï¼‰
        df = pd.DataFrame(keypoints_list, columns=multi_index)

        # å¥—ç”¨æ¿¾æ³¢
        for kp in coco_keypoints:
            for axis in ["x", "y"]:
                df[(scorer, kp, axis)] = butterworth_filter(df[(scorer, kp, axis)])

        # å¯«å…¥ HDF5
        df.to_hdf(h5_output_path, key="df_with_missing", mode="w")
        print(f"âœ… å·²å„²å­˜ HDF5ï¼š{h5_output_path}")

    except Exception as e:
        print(f"âŒ è½‰æ›å¤±æ•—ï¼š{json_path}ï¼ŒéŒ¯èª¤ï¼š{e}")

print("\nğŸ‰ æ‰€æœ‰ JSON å·²è½‰æ›ç‚º HDF5ï¼")


